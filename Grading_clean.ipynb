{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNX8E4TS/EYoNX0FMGtH2rc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehabsuliman/Essay-Grading-Transformer/blob/main/Grading_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvSFlc8pJYYy",
        "outputId": "04f6f83b-f0c5-46a4-c013-bcff5d916f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ehabsuliman/Essay-Grading-Transformer.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e56bfZxNKOmU",
        "outputId": "ad02ed78-9998-481e-d4f1-f4493d5d1790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Essay-Grading-Transformer' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Essay-Grading-Transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_C_wvQAKcr7",
        "outputId": "da4df0bd-cdeb-456a-82ff-d7613e3fe3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Essay-Grading-Transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "Jy5XVoepKm6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "ad3ddd1137384668a01d970db53b24ac",
            "aaa7be3a764c4a7fb4164fb95570cfae",
            "3b82feea16e74d4e8bb62113bcbfe546",
            "7fbff30480ff4f1fa355430a700ecc6d",
            "a4eafde595fb4b6c83ed215a0ebdca6e",
            "faaed422a45d489fa4c3326f0064980b",
            "d91c61f0dbcc427e8726d916467f4f1f",
            "267ca5e54c5a414abdc1f5b6a4e9938f",
            "b1710f3eebc04e3e82e1779d4eae7709",
            "c2443150bf66434aaa2e37e42883a347",
            "014db7841d524bcda33844c83c8de064",
            "e9235f9d2b3b4b668f4dccc90fc62478",
            "cea46744d8db407bb6d0083a388f5c76",
            "0faf0e87b76b403d8a93eef23f213730",
            "44c1803f49014b0fb8a1f40ba02dd3b8",
            "ddc73c56835742aaadd8b3b9116210cf",
            "026959b1517944f3a59486ca2d8bbb15",
            "9300b056f3824ae19d2736897dceb1f8",
            "1c7d9a268571407db09ac86478d5f3b3",
            "5d5789faeaaa43eeb1170507617ac4a9",
            "7378257a6e2c4ad8be326b0867e4da1a",
            "8e968c2986a54c71a7400f024799bb4a",
            "ff795d9192474d6cac9722d2520ad00c",
            "54e57d4880a641f49d6479faf59a9015",
            "64b0bbd40c9d41f5a167543971ac075d",
            "22d5d9867b5541a7ab3a4da15c7631ba",
            "bcad5c00646d4763b12fb3bf04c87855",
            "1e42300f3b034e8f81277eed492820d7",
            "d06101476fde4c7a99a64e0f5eec0ef3",
            "9b59fbe9690d4ff8acf26cfb3af3d9bc",
            "768b17c352d848c393dfdbba112ee7d5",
            "bccaa8746e674e21864b67f2b0522d1f",
            "173a53309a0740d69a4f7e997b41f58b",
            "0a533110246c411fa35f7c32349fa46c",
            "6faa7016d7a04e2683e7c53ecd1b52f6",
            "2fd86fb8089c4735af660b39f4bb7998",
            "ceb864b3f53342f0b15debdc72a975f9",
            "bc9a5e23bb9e4942b153e38a011abf0a",
            "1f9044113b6e4cb0b222ffb18236d51a",
            "59decb174b674340bb257483d03ec9b3",
            "97b1abfd63bb4ce2b3bc8352a0705a4a",
            "0d4375522879457bb0acd34f32e485ea",
            "45d2be3e3f45434ca7ce57cc1db5e0c9",
            "acb520565d11495895c7f1b511e0031c",
            "79cf5ed91956455f80052c560fc25694",
            "1941aaa6e99b48e6b69f7cc7ede86c4b",
            "62eb5ba7695e4e72a612d9de8933222f",
            "552504842aec49e4891173a197f10be7",
            "0698f3c5a7c24eaca7182a0a71a87024",
            "e436032838654e3e82712191ca5e9757",
            "de8c876b2fc34535be3dd3d952191e88",
            "d0a0257eba104461aebb9f350c7e8ea5",
            "021dbf4c635846a89bd27851f86d3deb",
            "c3fba1bfd18b464c87f3537cb3552acb",
            "b91dd536f41c4733bd75ecad398232c9",
            "5b9b3153b7fd470687fff742a16d6f64",
            "1ab97141f68b459187b3a5f0d15c5572",
            "a9eedef145714b74ad2d3006dff773bd",
            "4d672e505cc44f7fae1bf1a5fb6ffcef",
            "9b676401d7694215adf2ce9200a0b8a1",
            "7570b78f464844c88d85bf6749742926",
            "609f8b83612b4858b9fea4c1bb6c14e3",
            "c5420f249d9d43ada38cf3880b84ca3d",
            "6614d6ae663540379dfcd6e3f2d5a7b4",
            "97df0be4d8244021934f985b4de15ec3",
            "0c37e53011044800a56990fd66481e4f"
          ]
        },
        "id": "rGwDk9VqLoG1",
        "outputId": "b8b6b285-c5b4-4ff1-83b9-98b1816587cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.5.7: Fast Mistral patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad3ddd1137384668a01d970db53b24ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9235f9d2b3b4b668f4dccc90fc62478"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.13k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff795d9192474d6cac9722d2520ad00c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a533110246c411fa35f7c32349fa46c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79cf5ed91956455f80052c560fc25694"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b9b3153b7fd470687fff742a16d6f64"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZFsW4jVMYN3",
        "outputId": "f2d80446-2fd0-4241-c8b4-1d6ba179f5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.5.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is a prompt and a partial response. Complete the response appropriately.\n",
        "\n",
        "### Prompt:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n"
      ],
      "metadata": {
        "id": "grlzGx7CM3lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    prompts    = examples[\"prompt\"]\n",
        "    completions = examples[\"completion\"]\n",
        "    texts = []\n",
        "    for prompt, completion in zip(prompts, completions):\n",
        "        text = alpaca_prompt.format(prompt, completion) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\": texts }\n"
      ],
      "metadata": {
        "id": "j6cc4ZmnM7RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load your JSONL file\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/NLP_exam_score.jsonl\", split=\"train\")\n",
        "\n",
        "# Format the prompts using your custom function\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "f39fa39fb4614aaaa063e8332e52c00d",
            "32d012973c2042d9a08078fad13ddc2a",
            "b9a830919fa346d5a4fbba988435a7f5",
            "eb4cc6745f74458ab78cf88dc560d15a",
            "4ecbb56b65a04f4b8755cb1dc1871f14",
            "8ca1d92e8a7b480091476b116553ff0b",
            "e38b7adc796d407098f40912648aa10c",
            "d3c27c40f7674cf1a02008457f769931",
            "baed1d1ea92c43a8868e77b820498ced",
            "166fad1671654ce2a2fbaf4f1bc6d5c0",
            "b8100b93f3da418fabc344d62e394dab",
            "0540cfb58cf740309bc0df58ec4ca38f",
            "dcce4f14b4ae40f7aa2f87903de7e18b",
            "1b1f3b404d0e42e284680faa0279ab74",
            "4c1191bf82e84807bd4d6ab3ff2ce895",
            "80f22ce8e54a4f1a9943f162708a4ab3",
            "b1dedaa7e4974a0da997f5a676be72da",
            "9f804620265842c2986800a1786f9069",
            "e9f044e7abd0496898c8deaa796bb90b",
            "a41ba5e6042c40d9b70dfa8d2f2bed01",
            "087c6527b2d94af29d6037b15ffa1b1c",
            "8e1cd9866afc477c910bd786b27cc5bf"
          ]
        },
        "id": "aNLw9cCXM9Jn",
        "outputId": "fba213ff-9504-4b1a-d714-7c7cc32988d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f39fa39fb4614aaaa063e8332e52c00d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0540cfb58cf740309bc0df58ec4ca38f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60, # Set num_train_epochs = 1 for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c81c6529fb6a46adb4c0ddcc1ad8d15c",
            "86ec13c245fc43c2838d1e165df2ab90",
            "37161e16b94c4879a01c04524163509e",
            "ea92f4cef5e14240aa09cc37352c2f0d",
            "fc6d60fa7752433c80adafc0e4fa83fa",
            "79df80674ced44d1a7312e3bbc41c73a",
            "6d437a65cd04459c859244d23e638d16",
            "9f57b37bcc934c1ba0e0fe9e2aef2249",
            "e345616b230040408cb1f363419df1de",
            "209bede3663d49f2995de0e75b9db61b",
            "6f0587afcd744cf9b5d833d7218321b8"
          ]
        },
        "id": "6dDy2QJbND9J",
        "outputId": "fe108940-d2cf-4cf4-9f17-59284df65644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c81c6529fb6a46adb4c0ddcc1ad8d15c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oLn2LwbINHk8",
        "outputId": "34d38929-31fd-40c7-cbc0-436280f0c9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040/7,000,000,000 (0.60% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 05:55, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.373400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.435500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.173400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.684200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.345000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.161300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.987400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.859600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.743300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.730600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.739700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.649100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.713300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.676400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.659800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.612800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.647000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.676600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.574600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.515200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.592100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.561500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.476700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.551000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.513600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.551800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.533000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.544400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.467200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.474700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.455300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.462900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.456600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.451600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.429200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.386600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.428000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.402500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.359600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.380400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.392900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.397100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.408400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.338100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.389100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.343500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.318500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.323700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.329300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.331200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.361500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.340400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.317600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.296900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.322800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.286500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYa9U3euNIh4",
        "outputId": "44a0ae25-934d-40a2-d9fc-367ec6c4fc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "6.883 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os6d3ImoOz5G",
        "outputId": "49994131-054d-4806-8139-41db590bacb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "368.8467 seconds used for training.\n",
            "6.15 minutes used for training.\n",
            "Peak reserved memory = 6.883 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 46.693 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create folder in Drive\n",
        "!mkdir -p /content/drive/MyDrive/fine_tuned_model\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/fine_tuned_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9xrIePVO5xJ",
        "outputId": "eb2082e8-e1ba-4239-df82-fc8a62ef2a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/fine_tuned_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/fine_tuned_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/fine_tuned_model/tokenizer.model',\n",
              " '/content/drive/MyDrive/fine_tuned_model/added_tokens.json',\n",
              " '/content/drive/MyDrive/fine_tuned_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"ehab.n.suliman@gmail.com\"\n",
        "!git config --global user.name \"ehabsuliman\"\n",
        "\n"
      ],
      "metadata": {
        "id": "mWPVsUzTPIOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/Grading.ipynb\" \"/content/Essay-Grading-Transformer/\"\n"
      ],
      "metadata": {
        "id": "1vFFrsQGPWh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Essay-Grading-Transformer/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axbMFAkdRSxb",
        "outputId": "d7acb647-2379-49c4-fbcd-47c6be455b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Essay-Grading-Transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the notebook\n",
        "nb_path = \"/content/drive/MyDrive/Colab Notebooks/Grading.ipynb\"\n",
        "with open(nb_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb_data = json.load(f)\n",
        "\n",
        "# Remove corrupted widget metadata\n",
        "if 'widgets' in nb_data['metadata']:\n",
        "    del nb_data['metadata']['widgets']\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(\"/content/Grading_clean.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nb_data, f, indent=2)\n"
      ],
      "metadata": {
        "id": "9p5Xdts8SHuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/Grading_clean.ipynb /content/Essay-Grading-Transformer/\n",
        "%cd /content/Essay-Grading-Transformer/\n",
        "!git add Grading_clean.ipynb\n",
        "!git commit -m \"Fix invalid notebook metadata\"\n",
        "!git push\n"
      ],
      "metadata": {
        "id": "TFfs9dTbSMIP",
        "outputId": "7d62129f-158d-4bc8-ed2f-cba0cd100623",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Essay-Grading-Transformer\n",
            "[main 07b2cf3] Fix invalid notebook metadata\n",
            " 1 file changed, 1178 insertions(+)\n",
            " create mode 100644 Grading_clean.ipynb\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add Grading.ipynb\n",
        "!git commit -m \"finetuned unsloth mastral 7b 4bit model from huggingface\"\n",
        "!git push"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ALNvSOZPhCE",
        "outputId": "3ebe6219-ec71-4dec-c600-fb7635aad078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main f522ab5] finetuned unsloth mastral 7b 4bit model from huggingface\n",
            " 1 file changed, 1 insertion(+)\n",
            " create mode 100644 Grading.ipynb\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    }
  ]
}